import pandas as pd
#For removing HTML tags
from bs4 import BeautifulSoup
import string
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.tokenize import WhitespaceTokenizer
from nltk.stem import WordNetLemmatizer
import csv
from wordcloud import WordCloud
import matplotlib.pyplot as plt


#part of speech tagging
from nltk.corpus import wordnet

def get_wordnet_pos(pos_tag):
    if pos_tag.startswith('J'):
        return wordnet.ADJ
    elif pos_tag.startswith('V'):
        return wordnet.VERB
    elif pos_tag.startswith('N'):
        return wordnet.NOUN
    elif pos_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
        
#cleaning function
def clean(text):
    #remove html
    text = BeautifulSoup(text).get_text()
    #make words lowercase
    text = text.lower()   
    
    #remove any kind of punctuation
    text = [word.strip(string.punctuation) for word in text.split(" ")]
    #remove any kind of digits
    text = [word for word in text if not any(c.isdigit() for c in word)]
    #stop words
    stop = stopwords.words('english')
    text = [x for x in text if x not in stop]
    #empty tokens
    text = [t for t in text if len(t) > 0]
    #lemmatize
    pos_tags = pos_tag(text)
    text=[WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]
    
    text = [t for t in text if len(t)>1]
    
    text = " ".join(text)
    return text
    
for i in range(0,num):
    clean_text.append( clean(train["review"][i] ))
    if i%1000 == 0:
        print("Complete" +str(i))
train["ClReviews"]= clean_text


#checking distribution of the data
from numpy.random import seed
from numpy.random import randn
from matplotlib import pyplot

data["sentiment"].unique()
pyplot.hist(data["sentiment"])
pyplot.show()

from sklearn.model_selection import train_test_split
X = data.iloc[:,3].values
y = data.iloc[:,1].values

#splitting the dataset into train,test,cv
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)
X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, stratify=y_train)

#tfidf vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer 
from sklearn.naive_bayes import MultinomialNB
tf_vec = TfidfVectorizer()
X_train_tf = tf_vec.fit_transform(X_train)
X_cv_tf = tf_vec.fit_transform(X_cv)
X_test_tf = tf_vec.transform(X_test)
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train_tf,y_train)
y_pred = nb_classifier.predict(X_test_tf)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
cm = confusion_matrix(y_test,y_pred)
print(cm)
accuracy = accuracy_score(y_test,y_pred)
print(accuracy)

#randomized search CV
from sklearn.model_selection import GridSearchCV
from scipy.stats import randint as sp_randint
from sklearn.model_selection import RandomizedSearchCV
parameters = {  
'alpha': (0.001,0.01,0.1,1,5,10)  
} 

neigh = MultinomialNB(class_prior=[0.5,0.5])
clf = RandomizedSearchCV(neigh,parameters,  cv=10, scoring='roc_auc')
clf.fit(X_train_tf, y_train)

results = pd.DataFrame.from_dict(clf.cv_results_)
results.param_alpha
results = results.sort_values(['param_alpha'])
train_auc= results['mean_score_time']
train_auc_std= results['std_score_time']
cv_auc = results['mean_test_score'] 
cv_auc_std= results['std_test_score']
K =  results['param_alpha']


#plot AUC
plt.plot(K, train_auc, label='Train AUC')

plt.plot(K, cv_auc, label='CV AUC')

plt.scatter(K, train_auc, label='Train AUC points')

plt.scatter(K, cv_auc, label='CV AUC points')
plt.legend()
plt.xlabel("alpha: hyperparameter")
plt.ylabel("AUC")
plt.title("Hyper parameter Vs AUC plot")
plt.grid()
plt.show()

#roc curve
from sklearn.metrics import roc_curve, auc, roc_auc_score
import matplotlib.pyplot as plt

y_pred = [x[1] for x in nb_classifier.predict_proba(X_test_tf)]
fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label = 1)

roc_auc = auc(fpr, tpr)

plt.figure(1, figsize = (15, 10))
lw = 2
plt.plot(fpr, tpr, color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.show()






    
